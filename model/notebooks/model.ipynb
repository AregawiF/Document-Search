{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     EliteBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\HP\n",
      "[nltk_data]     EliteBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\HP\n",
      "[nltk_data]     EliteBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopset = set(stopwords.words('english')).union(\n",
    "                  {\"things\", \"that's\", \"something\", \"take\", \"don't\", \"may\", \"want\", \"you're\",\n",
    "                   \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\",\n",
    "                   \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\",\n",
    "                   \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\",\n",
    "                                                                  \"verb\", \"verbs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)      # Remove non-ASCII characters\n",
    "    text = re.sub(r'@\\w+', '', text)                # Remove email mentions, user handles, etc.\n",
    "    text = text.lower()                             # Lowercase text\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)            # Remove punctuation\n",
    "    text = re.sub(r'[0-9]', '', text)               # Remove numbers\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()     # Remove extra spaces\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(docs):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word', \n",
    "        ngram_range=(1, 2), \n",
    "        min_df=0.002, \n",
    "        max_df=0.99, \n",
    "        max_features=10000, \n",
    "        lowercase=True, \n",
    "        stop_words=list(english_stopset))\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    return X, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatize(word):\n",
    "    # Dictionary of custom word transformations\n",
    "    custom_lemmas = {\n",
    "        'ethiopian': 'ethiopia',\n",
    "        'ethiopia': 'ethiopia',}\n",
    "    return custom_lemmas.get(word.lower(), word)\n",
    "\n",
    "def lemmatize_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize each token and return the lemmatized text\n",
    "    lemmatized_text = \" \".join([custom_lemmatize(token.lemma_) for token in doc if token.text not in english_stopset])\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df, vectorizer, k, query, docs):\n",
    "    print(\"Done Searching. Full Result: \\n\")\n",
    "    print(\"searched items : \", query)\n",
    "    print(\"Article with the Highest Cosine Similarity Values: \")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    top_results=5\n",
    "    q = [q]\n",
    "\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)  \n",
    "\n",
    "    sim_sorted = sorted(sim.items(),key=lambda x : x[1], reverse=True)[:min(len(sim), top_results)]\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    for i, v in sim_sorted:    # Print the articles and their similarity values\n",
    "        if v != 0.0:\n",
    "            print(\"Similaritas score: \", v)\n",
    "        print(docs[i])\n",
    "        print('\\n')\n",
    "        n += 1\n",
    "        if n == k:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample document example\n",
    "docs = ['i loved you ethiopian, stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe',\n",
    "\n",
    "        'also, sometimes, the same words can have multiple different ‘lemma’s. So, based on the context it’s used, you should identify the \\\n",
    "        part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes \\\n",
    "        in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the \\\n",
    "        Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages',\n",
    "\n",
    "        'With more than  million people, ethiopia is the second most populous nation in Africa after Nigeria, and the fastest growing \\\n",
    "         economy in the region. However, it is also one of the poorest, with a per capita income',\n",
    "\n",
    "        'The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring\\\n",
    "         countries.ethiopia With a planned.',\n",
    "\n",
    "        'The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The \\\n",
    "         word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.',\n",
    "\n",
    "        'Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power\\\n",
    "         generation will start well before the completion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Searching. Full Result: \n",
      "\n",
      "searched items :  loved\n",
      "Article with the Highest Cosine Similarity Values: \n",
      "----------------------------------------------------\n",
      "Similaritas score:  0.19797091359850721\n",
      "Generator\n",
      "Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power         generation will start well before the completion\n",
      "\n",
      "\n",
      "Similaritas score:  0.19507975074710232\n",
      "Power Grid\n",
      "The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The          word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.\n",
      "\n",
      "\n",
      "Similaritas score:  0.15545752122343945\n",
      "Two upgraded\n",
      "i loved you ethiopian, stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe\n",
      "\n",
      "\n",
      "Done Searching. Full Result: \n",
      "\n",
      "searched items :  love\n",
      "Article with the Highest Cosine Similarity Values: \n",
      "----------------------------------------------------\n",
      "Similaritas score:  0.19797091359850721\n",
      "Generator\n",
      "Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power         generation will start well before the completion\n",
      "\n",
      "\n",
      "Similaritas score:  0.19507975074710232\n",
      "Power Grid\n",
      "The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The          word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.\n",
      "\n",
      "\n",
      "Similaritas score:  0.15545752122343945\n",
      "Two upgraded\n",
      "i loved you ethiopian, stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_docs = [clean_text(doc) for doc in docs]\n",
    "lemmatized_docs = [lemmatize_text_spacy(doc) for doc in cleaned_docs]\n",
    " \n",
    "X, vectorizer = vectorize_text(lemmatized_docs)\n",
    "k = 3\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.T.toarray())\n",
    "\n",
    "\n",
    "query1 = 'loved'\n",
    "query2 = 'love'\n",
    "q1 = lemmatize_text_spacy(query1)\n",
    "q2 = lemmatize_text_spacy(query2)\n",
    "\n",
    "\n",
    "get_similar_articles(q1, df, vectorizer, k, query1, docs)\n",
    "get_similar_articles(q2, df, vectorizer, k, query2, docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
